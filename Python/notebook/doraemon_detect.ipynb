{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>is_gadget</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Album dự báo tương lai</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ban nhạc cảm xúc thăng hoa</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bom tạo không gian</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bánh mì chuyển ngữ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bánh mì giúp trí nhớ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>Công nhân</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>Đồng hồ điện tử</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>Đồng hồ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>Ống sáo</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>Đàn piano</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>472 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           name  is_gadget\n",
       "0        Album dự báo tương lai          1\n",
       "1    Ban nhạc cảm xúc thăng hoa          1\n",
       "2            Bom tạo không gian          1\n",
       "3            Bánh mì chuyển ngữ          1\n",
       "4          Bánh mì giúp trí nhớ          1\n",
       "..                          ...        ...\n",
       "467                   Công nhân          0\n",
       "468             Đồng hồ điện tử          0\n",
       "469                     Đồng hồ          0\n",
       "470                     Ống sáo          0\n",
       "471                   Đàn piano          0\n",
       "\n",
       "[472 rows x 2 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('database/doraemon_detect.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>is_gadget</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>Cổng chào</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>Thuốc nói dóc</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>Tàu ngầm</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>Ông bụt Robot</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Khinh khí cầu nhỏ Mini</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       name  is_gadget\n",
       "289               Cổng chào          0\n",
       "174           Thuốc nói dóc          1\n",
       "430                Tàu ngầm          0\n",
       "213           Ông bụt Robot          1\n",
       "83   Khinh khí cầu nhỏ Mini          1"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "x = df['name']\n",
    "y = df['is_gadget']\n",
    "xtrain, xtest, ytrain, ytest = tts(x, y, stratify=y, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m tokenizer \u001b[39m=\u001b[39m BertTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mbert-base-uncased\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m model \u001b[39m=\u001b[39m BertModel\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mbert-base-uncased\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m texts \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mtolist()\n\u001b[0;32m      8\u001b[0m labels \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mis_gadget\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mtolist()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "texts = df['name'].tolist()\n",
    "labels = df['is_gadget'].tolist()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "BertTokenizer._tokenize() got an unexpected keyword argument 'pad_to_max_length'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[109], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tokenizer\u001b[39m.\u001b[39;49mencode(\u001b[39m\"\u001b[39;49m\u001b[39m我是一只猫\u001b[39;49m\u001b[39m\"\u001b[39;49m, add_special_tokens\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, max_length\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, pad_to_max_length\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils.py:728\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.encode\u001b[1;34m(self, text, text_pair, add_special_tokens, max_length, stride, truncation_strategy, return_tensors, **kwargs)\u001b[0m\n\u001b[0;32m    691\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencode\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[0;32m    692\u001b[0m             text,\n\u001b[0;32m    693\u001b[0m             text_pair\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    698\u001b[0m             return_tensors\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    699\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    700\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    701\u001b[0m \u001b[39m    Converts a string in a sequence of ids (integer), using the tokenizer and vocabulary.\u001b[39;00m\n\u001b[0;32m    702\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    726\u001b[0m \u001b[39m        **kwargs: passed to the `self.tokenize()` method\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 728\u001b[0m     encoded_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode_plus(text,\n\u001b[0;32m    729\u001b[0m                                       text_pair\u001b[39m=\u001b[39;49mtext_pair,\n\u001b[0;32m    730\u001b[0m                                       max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[0;32m    731\u001b[0m                                       add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[0;32m    732\u001b[0m                                       stride\u001b[39m=\u001b[39;49mstride,\n\u001b[0;32m    733\u001b[0m                                       truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[0;32m    734\u001b[0m                                       return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[0;32m    735\u001b[0m                                       \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    737\u001b[0m     \u001b[39mreturn\u001b[39;00m encoded_inputs[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils.py:786\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, max_length, stride, truncation_strategy, return_tensors, **kwargs)\u001b[0m\n\u001b[0;32m    783\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    784\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInput is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 786\u001b[0m first_ids \u001b[39m=\u001b[39m get_input_ids(text)\n\u001b[0;32m    787\u001b[0m second_ids \u001b[39m=\u001b[39m get_input_ids(text_pair) \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    789\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_for_model(first_ids,\n\u001b[0;32m    790\u001b[0m                               pair_ids\u001b[39m=\u001b[39msecond_ids,\n\u001b[0;32m    791\u001b[0m                               max_length\u001b[39m=\u001b[39mmax_length,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    794\u001b[0m                               truncation_strategy\u001b[39m=\u001b[39mtruncation_strategy,\n\u001b[0;32m    795\u001b[0m                               return_tensors\u001b[39m=\u001b[39mreturn_tensors)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils.py:778\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.encode_plus.<locals>.get_input_ids\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m    776\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_input_ids\u001b[39m(text):\n\u001b[0;32m    777\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(text, six\u001b[39m.\u001b[39mstring_types):\n\u001b[1;32m--> 778\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconvert_tokens_to_ids(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenize(text, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n\u001b[0;32m    779\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(text, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(text) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(text[\u001b[39m0\u001b[39m], six\u001b[39m.\u001b[39mstring_types):\n\u001b[0;32m    780\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconvert_tokens_to_ids(text)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils.py:649\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.tokenize\u001b[1;34m(self, text, **kwargs)\u001b[0m\n\u001b[0;32m    644\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msum\u001b[39m((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tokenize(token, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39mif\u001b[39;00m token \u001b[39mnot\u001b[39;00m \\\n\u001b[0;32m    645\u001b[0m             \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madded_tokens_encoder \u001b[39mand\u001b[39;00m token \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_special_tokens \\\n\u001b[0;32m    646\u001b[0m             \u001b[39melse\u001b[39;00m [token] \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m tokenized_text), [])\n\u001b[0;32m    648\u001b[0m added_tokens \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39madded_tokens_encoder\u001b[39m.\u001b[39mkeys()) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_special_tokens\n\u001b[1;32m--> 649\u001b[0m tokenized_text \u001b[39m=\u001b[39m split_on_tokens(added_tokens, text)\n\u001b[0;32m    650\u001b[0m \u001b[39mreturn\u001b[39;00m tokenized_text\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils.py:644\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.tokenize.<locals>.split_on_tokens\u001b[1;34m(tok_list, text)\u001b[0m\n\u001b[0;32m    641\u001b[0m             tokenized_text \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [sub_text]\n\u001b[0;32m    642\u001b[0m     text_list \u001b[39m=\u001b[39m tokenized_text\n\u001b[1;32m--> 644\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msum\u001b[39m((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tokenize(token, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39mif\u001b[39;00m token \u001b[39mnot\u001b[39;00m \\\n\u001b[0;32m    645\u001b[0m         \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madded_tokens_encoder \u001b[39mand\u001b[39;00m token \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_special_tokens \\\n\u001b[0;32m    646\u001b[0m         \u001b[39melse\u001b[39;00m [token] \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m tokenized_text), [])\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils.py:644\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    641\u001b[0m             tokenized_text \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [sub_text]\n\u001b[0;32m    642\u001b[0m     text_list \u001b[39m=\u001b[39m tokenized_text\n\u001b[1;32m--> 644\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msum\u001b[39m((\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tokenize(token, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs) \u001b[39mif\u001b[39;00m token \u001b[39mnot\u001b[39;00m \\\n\u001b[0;32m    645\u001b[0m         \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madded_tokens_encoder \u001b[39mand\u001b[39;00m token \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_special_tokens \\\n\u001b[0;32m    646\u001b[0m         \u001b[39melse\u001b[39;00m [token] \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m tokenized_text), [])\n",
      "\u001b[1;31mTypeError\u001b[0m: BertTokenizer._tokenize() got an unexpected keyword argument 'pad_to_max_length'"
     ]
    }
   ],
   "source": [
    "tokenizer.encode(\"我是一只猫\", add_special_tokens=True, max_length=10, pad_to_max_length=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
